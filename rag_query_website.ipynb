{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG-Based Query System for UB CSE Website\n",
        "\n",
        "This notebook implements a **Retrieval Augmented Generation (RAG)** system to query the large mirrored website.\n",
        "\n",
        "## The Problem\n",
        "- **4,042 HTML files** (~403MB) - too large for any LLM context window\n",
        "- Need to find relevant content before querying\n",
        "- Need to provide only relevant context to the LLM\n",
        "\n",
        "## The Solution: RAG Pipeline\n",
        "\n",
        "1. **Chunk** HTML files into smaller pieces\n",
        "2. **Embed** each chunk using embeddings model\n",
        "3. **Store** embeddings in a vector database\n",
        "4. **Retrieve** relevant chunks for a query\n",
        "5. **Generate** answer using LLM with retrieved context\n",
        "\n",
        "This allows querying the entire website efficiently!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Install Dependencies\n",
        "\n",
        "We'll need:\n",
        "- `ollama` - for LLM and embeddings\n",
        "- `chromadb` - lightweight vector database\n",
        "- `beautifulsoup4` & `html2text` - for HTML processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì ollama already installed\n",
            "Installing chromadb...\n",
            "‚úì chromadb installed\n",
            "‚úì beautifulsoup4 already installed\n",
            "Installing html2text...\n",
            "‚úì html2text installed\n",
            "\n",
            "‚úì All packages ready!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "packages = ['ollama', 'chromadb', 'beautifulsoup4', 'html2text']\n",
        "\n",
        "for package in packages:\n",
        "    try:\n",
        "        if package == 'chromadb':\n",
        "            __import__('chromadb')\n",
        "        elif package == 'beautifulsoup4':\n",
        "            __import__('bs4')\n",
        "        elif package == 'html2text':\n",
        "            __import__('html2text')\n",
        "        else:\n",
        "            __import__(package)\n",
        "        print(f\"‚úì {package} already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
        "        print(f\"‚úì {package} installed\")\n",
        "\n",
        "import ollama\n",
        "import chromadb\n",
        "from bs4 import BeautifulSoup\n",
        "import html2text\n",
        "import os\n",
        "from pathlib import Path\n",
        "import hashlib\n",
        "from typing import List, Dict\n",
        "\n",
        "print(\"\\n‚úì All packages ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Configuration\n",
        "\n",
        "Set up paths and model choices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mirror folder: engineering.buffalo.edu\n",
            "Vector DB path: ./chroma_db\n",
            "Embedding model: nomic-embed-text\n",
            "LLM model: llama3.2:latest\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "MIRROR_FOLDER = \"engineering.buffalo.edu\"\n",
        "CHROMA_DB_PATH = \"./chroma_db\"  # Where to store the vector database\n",
        "\n",
        "# Model choices\n",
        "# For embeddings: nomic-embed-text is good and small\n",
        "# For querying: any Ollama model (llama3, mistral, etc.)\n",
        "EMBEDDING_MODEL = \"nomic-embed-text\"  # Good embedding model for Ollama\n",
        "LLM_MODEL = \"llama3.2:latest\"  # Change to your preferred model\n",
        "\n",
        "# Chunking settings\n",
        "CHUNK_SIZE = 1000  # Characters per chunk\n",
        "CHUNK_OVERLAP = 200  # Overlap between chunks\n",
        "\n",
        "print(f\"Mirror folder: {MIRROR_FOLDER}\")\n",
        "print(f\"Vector DB path: {CHROMA_DB_PATH}\")\n",
        "print(f\"Embedding model: {EMBEDDING_MODEL}\")\n",
        "print(f\"LLM model: {LLM_MODEL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) HTML Processing Functions\n",
        "\n",
        "Functions to extract clean text from HTML files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4042 HTML files\n",
            "\n",
            "Sample file: engineering.buffalo.edu/computer-science-engineering/information-for-faculty-and-staff.html\n",
            "Extracted text length: 15620 characters\n",
            "Preview: #  Information for Faculty and Staff \n",
            "\n",
            "[ Davis Hall by moonlight.  ](http://engineering.buffalo.edu/content/engineering/computer-science-engineering/information-for-faculty-and-staff/_jcr_content/par/...\n"
          ]
        }
      ],
      "source": [
        "# Initialize HTML to text converter\n",
        "h = html2text.HTML2Text()\n",
        "h.ignore_links = False\n",
        "h.ignore_images = True\n",
        "h.body_width = 0\n",
        "\n",
        "def extract_text_from_html(file_path: str) -> str:\n",
        "    \"\"\"Extract clean text from HTML file.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            html_content = f.read()\n",
        "        \n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        \n",
        "        # Remove script, style, and other non-content elements\n",
        "        for element in soup(['script', 'style', 'meta', 'link', 'nav', 'footer', 'header']):\n",
        "            element.decompose()\n",
        "        \n",
        "        # Get text\n",
        "        text = h.handle(str(soup))\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error reading {file_path}: {e}\"\n",
        "\n",
        "def find_html_files(root_dir: str) -> List[str]:\n",
        "    \"\"\"Find all HTML files in directory.\"\"\"\n",
        "    html_files = []\n",
        "    root_path = Path(root_dir)\n",
        "    \n",
        "    for html_file in root_path.rglob(\"*.html\"):\n",
        "        html_files.append(str(html_file))\n",
        "    \n",
        "    return html_files\n",
        "\n",
        "# Test\n",
        "if os.path.exists(MIRROR_FOLDER):\n",
        "    html_files = find_html_files(MIRROR_FOLDER)\n",
        "    print(f\"Found {len(html_files)} HTML files\")\n",
        "    \n",
        "    # Test extraction on one file\n",
        "    if html_files:\n",
        "        sample_text = extract_text_from_html(html_files[0])\n",
        "        print(f\"\\nSample file: {html_files[0]}\")\n",
        "        print(f\"Extracted text length: {len(sample_text)} characters\")\n",
        "        print(f\"Preview: {sample_text[:200]}...\")\n",
        "else:\n",
        "    print(f\"‚ö† {MIRROR_FOLDER} folder not found!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Chunking Strategy\n",
        "\n",
        "Split large HTML files into smaller chunks that fit in context windows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing chunking on first 5 files...\n",
            "Created 90 chunks from 5 files\n",
            "Average chunk size: 970 characters\n"
          ]
        }
      ],
      "source": [
        "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:\n",
        "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "    if len(text) <= chunk_size:\n",
        "        return [text]\n",
        "    \n",
        "    chunks = []\n",
        "    start = 0\n",
        "    \n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        \n",
        "        # Move start forward, accounting for overlap\n",
        "        start = end - overlap\n",
        "        \n",
        "        # Prevent infinite loop\n",
        "        if start >= len(text) - overlap:\n",
        "            break\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "def create_chunks_from_files(html_files: List[str], max_files: int = None) -> List[Dict]:\n",
        "    \"\"\"Create chunks from HTML files with metadata.\"\"\"\n",
        "    chunks = []\n",
        "    \n",
        "    files_to_process = html_files[:max_files] if max_files else html_files\n",
        "    \n",
        "    for file_path in files_to_process:\n",
        "        text = extract_text_from_html(file_path)\n",
        "        \n",
        "        if text and not text.startswith(\"Error\"):\n",
        "            file_chunks = chunk_text(text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
        "            \n",
        "            for i, chunk in enumerate(file_chunks):\n",
        "                chunks.append({\n",
        "                    'text': chunk,\n",
        "                    'file_path': file_path,\n",
        "                    'chunk_index': i,\n",
        "                    'total_chunks': len(file_chunks)\n",
        "                })\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# Test chunking\n",
        "if os.path.exists(MIRROR_FOLDER):\n",
        "    html_files = find_html_files(MIRROR_FOLDER)\n",
        "    print(f\"Testing chunking on first 5 files...\")\n",
        "    test_chunks = create_chunks_from_files(html_files[:5])\n",
        "    print(f\"Created {len(test_chunks)} chunks from 5 files\")\n",
        "    if test_chunks:\n",
        "        print(f\"Average chunk size: {sum(len(c['text']) for c in test_chunks) / len(test_chunks):.0f} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Create Vector Database\n",
        "\n",
        "This is the core of RAG - we'll:\n",
        "1. Create embeddings for each chunk\n",
        "2. Store them in ChromaDB\n",
        "3. This allows fast semantic search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Vector DB functions ready!\n"
          ]
        }
      ],
      "source": [
        "def get_embedding(text: str, model: str = EMBEDDING_MODEL) -> List[float]:\n",
        "    \"\"\"Get embedding for text using Ollama.\"\"\"\n",
        "    try:\n",
        "        response = ollama.embeddings(model=model, prompt=text)\n",
        "        return response['embedding']\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting embedding: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_vector_db(chunks: List[Dict], collection_name: str = \"website_chunks\"):\n",
        "    \"\"\"Create and populate ChromaDB vector database.\"\"\"\n",
        "    \n",
        "    # Initialize ChromaDB\n",
        "    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
        "    \n",
        "    # Create or get collection\n",
        "    try:\n",
        "        collection = client.get_collection(name=collection_name)\n",
        "        print(f\"Using existing collection: {collection_name}\")\n",
        "        print(f\"Current documents: {collection.count()}\")\n",
        "    except:\n",
        "        collection = client.create_collection(\n",
        "            name=collection_name,\n",
        "            metadata={\"hnsw:space\": \"cosine\"}\n",
        "        )\n",
        "        print(f\"Created new collection: {collection_name}\")\n",
        "    \n",
        "    # Process chunks in batches\n",
        "    batch_size = 10\n",
        "    total_chunks = len(chunks)\n",
        "    \n",
        "    print(f\"\\nProcessing {total_chunks} chunks...\")\n",
        "    \n",
        "    for i in range(0, total_chunks, batch_size):\n",
        "        batch = chunks[i:i+batch_size]\n",
        "        \n",
        "        ids = []\n",
        "        texts = []\n",
        "        embeddings = []\n",
        "        metadatas = []\n",
        "        \n",
        "        for chunk in batch:\n",
        "            # Create unique ID\n",
        "            chunk_id = hashlib.md5(\n",
        "                f\"{chunk['file_path']}_{chunk['chunk_index']}\".encode()\n",
        "            ).hexdigest()\n",
        "            \n",
        "            # Get embedding\n",
        "            embedding = get_embedding(chunk['text'], EMBEDDING_MODEL)\n",
        "            \n",
        "            if embedding:\n",
        "                ids.append(chunk_id)\n",
        "                texts.append(chunk['text'])\n",
        "                embeddings.append(embedding)\n",
        "                metadatas.append({\n",
        "                    'file_path': chunk['file_path'],\n",
        "                    'chunk_index': str(chunk['chunk_index']),\n",
        "                    'total_chunks': str(chunk['total_chunks'])\n",
        "                })\n",
        "        \n",
        "        if ids:\n",
        "            collection.add(\n",
        "                ids=ids,\n",
        "                documents=texts,\n",
        "                embeddings=embeddings,\n",
        "                metadatas=metadatas\n",
        "            )\n",
        "        \n",
        "        if (i + batch_size) % 100 == 0 or i + batch_size >= total_chunks:\n",
        "            print(f\"  Processed {min(i + batch_size, total_chunks)}/{total_chunks} chunks\")\n",
        "    \n",
        "    print(f\"\\n‚úì Vector database created with {collection.count()} chunks\")\n",
        "    return collection\n",
        "\n",
        "print(\"‚úì Vector DB functions ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 0: Pull Embedding Model (Required First!)\n",
        "\n",
        "Before building the index, you need to pull the embedding model. Run this cell first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if embedding model is available, if not, provide instructions\n",
        "try:\n",
        "    test_embedding = get_embedding(\"test\", EMBEDDING_MODEL)\n",
        "    if test_embedding:\n",
        "        print(f\"‚úì Embedding model '{EMBEDDING_MODEL}' is ready!\")\n",
        "        print(f\"  Embedding dimension: {len(test_embedding)}\")\n",
        "    else:\n",
        "        print(f\"‚ö† Embedding model '{EMBEDDING_MODEL}' not found.\")\n",
        "        print(f\"\\nPlease run this command in your terminal:\")\n",
        "        print(f\"  ollama pull {EMBEDDING_MODEL}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö† Embedding model '{EMBEDDING_MODEL}' not available.\")\n",
        "    print(f\"\\nPlease run this command in your terminal:\")\n",
        "    print(f\"  ollama pull {EMBEDDING_MODEL}\")\n",
        "    print(f\"\\nThen re-run this cell to verify it's installed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Build the Index (One-Time Setup)\n",
        "\n",
        "**‚ö†Ô∏è This will take time!** For 4,000+ files, this might take 30-60 minutes.\n",
        "\n",
        "You can:\n",
        "1. **Test first** with a small subset (e.g., 100 files)\n",
        "2. **Run full index** when ready\n",
        "3. **Resume later** - ChromaDB persists, so you can add more files incrementally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total HTML files: 4042\n",
            "\n",
            "üß™ TEST MODE: Indexing first 100 files\n",
            "\n",
            "Step 1: Creating chunks...\n",
            "Created 1038 chunks\n",
            "\n",
            "Checking embedding model: nomic-embed-text\n",
            "‚úì Embedding model ready (embedding dimension: 768)\n",
            "\n",
            "Step 2: Creating embeddings and vector database...\n",
            "This will take a while - grab a coffee! ‚òï\n",
            "Created new collection: website_chunks\n",
            "\n",
            "Processing 1038 chunks...\n",
            "  Processed 100/1038 chunks\n",
            "  Processed 200/1038 chunks\n",
            "  Processed 300/1038 chunks\n",
            "  Processed 400/1038 chunks\n",
            "  Processed 500/1038 chunks\n",
            "  Processed 600/1038 chunks\n",
            "  Processed 700/1038 chunks\n",
            "  Processed 800/1038 chunks\n",
            "  Processed 900/1038 chunks\n",
            "  Processed 1000/1038 chunks\n",
            "  Processed 1038/1038 chunks\n",
            "\n",
            "‚úì Vector database created with 1038 chunks\n",
            "\n",
            "‚úì Indexing complete! You can now proceed to query the database.\n"
          ]
        }
      ],
      "source": [
        "# OPTION 1: Test with small subset (recommended first!)\n",
        "TEST_MODE = True  # Set to False for full index\n",
        "TEST_FILE_COUNT = 100  # Number of files to index for testing\n",
        "\n",
        "if os.path.exists(MIRROR_FOLDER):\n",
        "    html_files = find_html_files(MIRROR_FOLDER)\n",
        "    print(f\"Total HTML files: {len(html_files)}\")\n",
        "    \n",
        "    if TEST_MODE:\n",
        "        print(f\"\\nüß™ TEST MODE: Indexing first {TEST_FILE_COUNT} files\")\n",
        "        files_to_index = html_files[:TEST_FILE_COUNT]\n",
        "    else:\n",
        "        print(f\"\\nüöÄ FULL MODE: Indexing all {len(html_files)} files\")\n",
        "        files_to_index = html_files\n",
        "    \n",
        "    # Create chunks\n",
        "    print(\"\\nStep 1: Creating chunks...\")\n",
        "    chunks = create_chunks_from_files(files_to_index)\n",
        "    print(f\"Created {len(chunks)} chunks\")\n",
        "    \n",
        "    # First, make sure embedding model is available\n",
        "    print(f\"\\nChecking embedding model: {EMBEDDING_MODEL}\")\n",
        "    test_embedding = None\n",
        "    try:\n",
        "        test_embedding = get_embedding(\"test\", EMBEDDING_MODEL)\n",
        "        if test_embedding:\n",
        "            print(f\"‚úì Embedding model ready (embedding dimension: {len(test_embedding)})\")\n",
        "        else:\n",
        "            print(f\"‚ö† Embedding model not available.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Error with embedding model: {e}\")\n",
        "    \n",
        "    # Only build index if embedding model is available\n",
        "    if test_embedding:\n",
        "        # Build the index\n",
        "        print(\"\\nStep 2: Creating embeddings and vector database...\")\n",
        "        print(\"This will take a while - grab a coffee! ‚òï\")\n",
        "        collection = create_vector_db(chunks)\n",
        "        \n",
        "        print(\"\\n‚úì Indexing complete! You can now proceed to query the database.\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö† Cannot build index - embedding model '{EMBEDDING_MODEL}' not available.\")\n",
        "        print(f\"\\nPlease run this command in your terminal:\")\n",
        "        print(f\"  ollama pull {EMBEDDING_MODEL}\")\n",
        "        print(f\"\\nThen re-run this cell to build the index.\")\n",
        "else:\n",
        "    print(f\"‚ö† {MIRROR_FOLDER} folder not found!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Query Function (RAG Pipeline)\n",
        "\n",
        "This is where the magic happens:\n",
        "1. Convert query to embedding\n",
        "2. Find similar chunks in vector DB\n",
        "3. Retrieve top-k chunks\n",
        "4. Pass to LLM with context\n",
        "5. Get answer!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Query function ready!\n"
          ]
        }
      ],
      "source": [
        "def query_website(query: str, collection, top_k: int = 5, model: str = LLM_MODEL):\n",
        "    \"\"\"Query the website using RAG.\"\"\"\n",
        "    \n",
        "    # Step 1: Get query embedding\n",
        "    print(f\"üîç Query: {query}\")\n",
        "    print(\"Step 1: Getting query embedding...\")\n",
        "    query_embedding = get_embedding(query, EMBEDDING_MODEL)\n",
        "    \n",
        "    if not query_embedding:\n",
        "        return \"Error: Could not get query embedding\"\n",
        "    \n",
        "    # Step 2: Search vector database\n",
        "    print(f\"Step 2: Searching vector database (top {top_k} results)...\")\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=top_k\n",
        "    )\n",
        "    \n",
        "    # Step 3: Prepare context\n",
        "    print(\"Step 3: Preparing context for LLM...\")\n",
        "    context_chunks = []\n",
        "    for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
        "        context_chunks.append(f\"\\n--- Chunk {i+1} from {metadata['file_path']} ---\\n{doc}\")\n",
        "    \n",
        "    context = \"\\n\".join(context_chunks)\n",
        "    \n",
        "    # Step 4: Query LLM with context\n",
        "    print(f\"Step 4: Querying LLM ({model})...\")\n",
        "    \n",
        "    prompt = f\"\"\"You are a helpful assistant answering questions about the University at Buffalo Computer Science and Engineering department website.\n",
        "\n",
        "Use the following context from the website to answer the question. If the answer is not in the context, say so.\n",
        "\n",
        "Context from website:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer based on the context:\"\"\"\n",
        "    \n",
        "    try:\n",
        "        response = ollama.generate(\n",
        "            model=model,\n",
        "            prompt=prompt\n",
        "        )\n",
        "        \n",
        "        answer = response['response']\n",
        "        \n",
        "        # Step 5: Return answer with sources\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"ANSWER:\")\n",
        "        print(\"=\"*70)\n",
        "        print(answer)\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"SOURCES:\")\n",
        "        print(\"=\"*70)\n",
        "        for i, metadata in enumerate(results['metadatas'][0], 1):\n",
        "            print(f\"{i}. {metadata['file_path']}\")\n",
        "        \n",
        "        return {\n",
        "            'answer': answer,\n",
        "            'sources': [m['file_path'] for m in results['metadatas'][0]]\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return f\"Error querying LLM: {e}\"\n",
        "\n",
        "print(\"‚úì Query function ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Load Vector Database and Query\n",
        "\n",
        "Now you can query the website!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Loaded vector database\n",
            "  Documents: 1038\n"
          ]
        }
      ],
      "source": [
        "# Load the vector database\n",
        "client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
        "\n",
        "try:\n",
        "    collection = client.get_collection(name=\"website_chunks\")\n",
        "    print(f\"‚úì Loaded vector database\")\n",
        "    print(f\"  Documents: {collection.count()}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö† Vector database not found: {e}\")\n",
        "    print(\"Run the indexing cells above first!\")\n",
        "    collection = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Query: What are the main research areas in the Computer Science department?\n",
            "Step 1: Getting query embedding...\n",
            "Step 2: Searching vector database (top 5 results)...\n",
            "Step 3: Preparing context for LLM...\n",
            "Step 4: Querying LLM (llama3.2:latest)...\n",
            "\n",
            "======================================================================\n",
            "ANSWER:\n",
            "======================================================================\n",
            "According to Chunk 2 from engineering.buffalo.edu/computer-science-engineering/research/research-areas.html, there are 18 research areas in the Computer Science and Engineering department. However, these 18 areas are grouped into four categories:\n",
            "\n",
            "1. Artificial Intelligence\n",
            "2. Programming Languages and Software Engineering\n",
            "3. Theory (subdivided into Algorithms and Complexity, Computer Security and Cryptography, Interdisciplinary, etc.)\n",
            "4. Interdisciplinary (subdivided into Computational Biology and Bioinformatics, Computing Education, Human-Computer Interaction, Society and Computing, etc.)\n",
            "\n",
            "These categories are not listed in order of priority or importance, but rather provide a way to organize the various research areas within the department.\n",
            "\n",
            "======================================================================\n",
            "SOURCES:\n",
            "======================================================================\n",
            "1. engineering.buffalo.edu/computer-science-engineering/research.html\n",
            "2. engineering.buffalo.edu/computer-science-engineering/research/research-areas.html\n",
            "3. engineering.buffalo.edu/computer-science-engineering/sitemap.html\n",
            "4. engineering.buffalo.edu/computer-science-engineering/research/research-centers-institutes-labs-and-groups.html\n",
            "5. engineering.buffalo.edu/computer-science-engineering/sitemap.html\n",
            "\n",
            "======================================================================\n",
            "\n",
            "üîç Query: Who are the faculty members?\n",
            "Step 1: Getting query embedding...\n",
            "Step 2: Searching vector database (top 5 results)...\n",
            "Step 3: Preparing context for LLM...\n",
            "Step 4: Querying LLM (llama3.2:latest)...\n",
            "\n",
            "======================================================================\n",
            "ANSWER:\n",
            "======================================================================\n",
            "Based on the provided context from the University at Buffalo Computer Science and Engineering department website, it appears that there are several faculty members mentioned. However, only a few names are explicitly mentioned.\n",
            "\n",
            "Specifically, the following faculty members are mentioned:\n",
            "\n",
            "1. David Doermann - referred to as the \"Empire Innovation Professor\" in the Department of Computer Science and Engineering.\n",
            "2. Kenneth Joseph - an assistant professor in the UB's Department of Computer Science and Engineering.\n",
            "\n",
            "Additionally, the website mentions that two faculty members in the School of Engineering and Applied Sciences received NSF CAREER Awards, but their names are not provided.\n",
            "\n",
            "It is also mentioned that 20 new faculty members joined the School of Engineering and Applied Sciences this semester, but their names are not listed.\n",
            "\n",
            "======================================================================\n",
            "SOURCES:\n",
            "======================================================================\n",
            "1. engineering.buffalo.edu/computer-science-engineering/about-us.html\n",
            "2. engineering.buffalo.edu/computer-science-engineering/apply-now.html\n",
            "3. engineering.buffalo.edu/computer-science-engineering/research/research-policies.html\n",
            "4. engineering.buffalo.edu/computer-science-engineering/news-and-events/news.html?par_newslist_start=500.html\n",
            "5. engineering.buffalo.edu/computer-science-engineering/news-and-events/news.html?par_newslist_start=610.html\n"
          ]
        }
      ],
      "source": [
        "# Example queries\n",
        "if collection:\n",
        "    # Query 1\n",
        "    result1 = query_website(\n",
        "        \"What are the main research areas in the Computer Science department?\",\n",
        "        collection,\n",
        "        top_k=5\n",
        "    )\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "    \n",
        "    # Query 2\n",
        "    result2 = query_website(\n",
        "        \"Who are the faculty members?\",\n",
        "        collection,\n",
        "        top_k=5\n",
        "    )\n",
        "else:\n",
        "    print(\"‚ö† Vector database not loaded. Build the index first!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Interactive Query Function\n",
        "\n",
        "Easy function to ask questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Query: What undergraduate programs are offered?\n",
            "Step 1: Getting query embedding...\n",
            "Step 2: Searching vector database (top 5 results)...\n",
            "Step 3: Preparing context for LLM...\n",
            "Step 4: Querying LLM (llama3.2:latest)...\n",
            "\n",
            "======================================================================\n",
            "ANSWER:\n",
            "======================================================================\n",
            "According to the context, the following undergraduate programs are offered by the University at Buffalo Computer Science and Engineering department:\n",
            "\n",
            "1. BS/MS in Computer Science and Engineering\n",
            "2. BS in Computer Engineering\n",
            "3. BS in Computer Science\n",
            "4. BA in Computer Science\n",
            "5. Interdisciplinary Undergraduate Programs (including a concentration in Bioinformatics and Computational Biology)\n",
            "6. BA in Social Sciences with an interdisciplinary Cognitive Science Concentration\n",
            "\n",
            "======================================================================\n",
            "SOURCES:\n",
            "======================================================================\n",
            "1. engineering.buffalo.edu/computer-science-engineering/academics.html\n",
            "2. engineering.buffalo.edu/computer-science-engineering/sitemap.html\n",
            "3. engineering.buffalo.edu/computer-science-engineering/undergraduate.html\n",
            "4. engineering.buffalo.edu/computer-science-engineering/academics.html\n",
            "5. engineering.buffalo.edu/computer-science-engineering/undergraduate.html\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'answer': 'According to the context, the following undergraduate programs are offered by the University at Buffalo Computer Science and Engineering department:\\n\\n1. BS/MS in Computer Science and Engineering\\n2. BS in Computer Engineering\\n3. BS in Computer Science\\n4. BA in Computer Science\\n5. Interdisciplinary Undergraduate Programs (including a concentration in Bioinformatics and Computational Biology)\\n6. BA in Social Sciences with an interdisciplinary Cognitive Science Concentration',\n",
              " 'sources': ['engineering.buffalo.edu/computer-science-engineering/academics.html',\n",
              "  'engineering.buffalo.edu/computer-science-engineering/sitemap.html',\n",
              "  'engineering.buffalo.edu/computer-science-engineering/undergraduate.html',\n",
              "  'engineering.buffalo.edu/computer-science-engineering/academics.html',\n",
              "  'engineering.buffalo.edu/computer-science-engineering/undergraduate.html']}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def ask(question: str, top_k: int = 5):\n",
        "    \"\"\"Simple function to ask questions.\"\"\"\n",
        "    if collection is None:\n",
        "        print(\"‚ö† Vector database not loaded. Run the cell above first!\")\n",
        "        return\n",
        "    \n",
        "    return query_website(question, collection, top_k=top_k, model=LLM_MODEL)\n",
        "\n",
        "# Example:\n",
        "ask(\"What undergraduate programs are offered?\")\n",
        "# ask(\"Tell me about the graduate program requirements\")\n",
        "# ask(\"What research labs are there?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: How RAG Works Here\n",
        "\n",
        "### The Problem\n",
        "- 4,042 HTML files = too much for LLM context\n",
        "- Need to find relevant info efficiently\n",
        "\n",
        "### The RAG Solution\n",
        "\n",
        "1. **Indexing Phase** (one-time, takes ~30-60 min):\n",
        "   - Extract text from HTML\n",
        "   - Chunk into smaller pieces\n",
        "   - Create embeddings (vector representations)\n",
        "   - Store in vector database\n",
        "\n",
        "2. **Query Phase** (fast, ~5-10 seconds):\n",
        "   - Convert your question to embedding\n",
        "   - Find similar chunks (semantic search)\n",
        "   - Retrieve top-k most relevant chunks\n",
        "   - Pass chunks + question to LLM\n",
        "   - Get answer with sources!\n",
        "\n",
        "### Advantages\n",
        "- ‚úÖ Handles large datasets\n",
        "- ‚úÖ Fast queries (only relevant chunks)\n",
        "- ‚úÖ Provides sources\n",
        "- ‚úÖ Works with local models (Ollama)\n",
        "- ‚úÖ Can update incrementally\n",
        "\n",
        "### Next Steps\n",
        "1. Pull embedding model: `ollama pull nomic-embed-text`\n",
        "2. Run indexing (start with test mode)\n",
        "3. Query away!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "AI",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
