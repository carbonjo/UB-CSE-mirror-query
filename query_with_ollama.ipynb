{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Query UB CSE Mirrored Website with Ollama\n",
        "\n",
        "This notebook allows you to query the locally mirrored `engineering.buffalo.edu` website using a local Ollama model.\n",
        "\n",
        "**Features:**\n",
        "- Choose from available Ollama models\n",
        "- Query the mirrored website content\n",
        "- Search and analyze HTML files\n",
        "- Extract information using local LLM models\n",
        "\n",
        "**Prerequisites:**\n",
        "- Ollama installed and running\n",
        "- The `engineering.buffalo.edu` folder from the mirroring workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Install and Setup Ollama\n",
        "\n",
        "### Install Ollama (if not already installed)\n",
        "\n",
        "If you don't have Ollama installed, you can install it from [ollama.com](https://ollama.com) or use Homebrew:\n",
        "\n",
        "```bash\n",
        "brew install ollama\n",
        "```\n",
        "\n",
        "### Start Ollama service\n",
        "\n",
        "Make sure Ollama is running. You can start it with:\n",
        "\n",
        "```bash\n",
        "ollama serve\n",
        "```\n",
        "\n",
        "Or if it's installed as a service, it should already be running.\n",
        "\n",
        "### Install Python dependencies\n",
        "\n",
        "We'll use the `ollama` Python package to interact with Ollama models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì ollama package already installed\n"
          ]
        }
      ],
      "source": [
        "# Install ollama Python package if not already installed\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    import ollama\n",
        "    print(\"‚úì ollama package already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing ollama package...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ollama\"])\n",
        "    import ollama\n",
        "    print(\"‚úì ollama package installed successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Check Ollama Connection and List Available Models\n",
        "\n",
        "First, let's verify Ollama is running and see what models are available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available Ollama models:\n",
            "--------------------------------------------------\n",
            "  ‚Ä¢ nomic-embed-text:latest (0.26 GB)\n",
            "  ‚Ä¢ llama3.2-vision:latest (7.28 GB)\n",
            "  ‚Ä¢ gemma3:4b (3.11 GB)\n",
            "  ‚Ä¢ llama4:latest (62.81 GB)\n",
            "  ‚Ä¢ llama3.2-vision:11b (7.36 GB)\n",
            "  ‚Ä¢ llama3.2:latest (1.88 GB)\n",
            "  ‚Ä¢ llama3.1:latest (4.34 GB)\n",
            "  ‚Ä¢ mistral:latest (3.83 GB)\n",
            "  ‚Ä¢ llama3:latest (4.34 GB)\n"
          ]
        }
      ],
      "source": [
        "# Check Ollama connection and list available models\n",
        "import ollama\n",
        "\n",
        "try:\n",
        "    # List available models\n",
        "    models = ollama.list()\n",
        "    print(\"Available Ollama models:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    if models.get('models'):\n",
        "        for model in models['models']:\n",
        "            model_name = model.get('model', 'Unknown')\n",
        "            model_size = model.get('size', 0)\n",
        "            size_gb = model_size / (1024**3) if model_size > 0 else 0\n",
        "            print(f\"  ‚Ä¢ {model_name} ({size_gb:.2f} GB)\")\n",
        "    else:\n",
        "        print(\"  No models found. Pull a model first:\")\n",
        "        print(\"  Example: ollama pull llama2\")\n",
        "        print(\"  Example: ollama pull mistral\")\n",
        "        print(\"  Example: ollama pull codellama\")\n",
        "except Exception as e:\n",
        "    print(f\"Error connecting to Ollama: {e}\")\n",
        "    print(\"\\nMake sure Ollama is running:\")\n",
        "    print(\"  Run 'ollama serve' in a terminal, or\")\n",
        "    print(\"  Install Ollama from https://ollama.com\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Choose a Model\n",
        "\n",
        "Available models are listed below with labels (model_1, model_2, etc.). \n",
        "\n",
        "To select a model, set `MODEL_NAME` to one of the model labels (e.g., `MODEL_NAME = \"model_1\"`).\n",
        "\n",
        "**Note:** If you don't have a model yet, you can pull it using:\n",
        "```bash\n",
        "ollama pull <model-name>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available models:\n",
            "------------------------------------------------------------\n",
            "  model_1      = nomic-embed-text:latest (0.26 GB)\n",
            "  model_2      = llama3.2-vision:latest (7.28 GB)\n",
            "  model_3      = gemma3:4b (3.11 GB)\n",
            "  model_4      = llama4:latest (62.81 GB)\n",
            "  model_5      = llama3.2-vision:11b (7.36 GB)\n",
            "  model_6      = llama3.2:latest (1.88 GB)\n",
            "  model_7      = llama3.1:latest (4.34 GB)\n",
            "  model_8      = mistral:latest (3.83 GB)\n",
            "  model_9      = llama3:latest (4.34 GB)\n",
            "------------------------------------------------------------\n",
            "\n",
            "Total: 9 models available\n",
            "\n",
            "To select a model, set MODEL_NAME to one of the labels above.\n",
            "Example: MODEL_NAME = 'model_1'\n",
            "\n",
            "‚úì Default model set to: model_1 (nomic-embed-text:latest)\n",
            "  Change MODEL_NAME to use a different model.\n"
          ]
        }
      ],
      "source": [
        "# List available models and create numbered labels\n",
        "try:\n",
        "    models = ollama.list()\n",
        "    available_models = [m['model'] for m in models.get('models', [])]\n",
        "    \n",
        "    if available_models:\n",
        "        # Create a dictionary mapping model labels to model names\n",
        "        model_dict = {}\n",
        "        print(\"Available models:\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        for i, model_name in enumerate(available_models, 1):\n",
        "            label = f\"model_{i}\"\n",
        "            model_dict[label] = model_name\n",
        "            model_size = next((m.get('size', 0) for m in models.get('models', []) if m.get('model') == model_name), 0)\n",
        "            size_gb = model_size / (1024**3) if model_size > 0 else 0\n",
        "            print(f\"  {label:12} = {model_name} ({size_gb:.2f} GB)\")\n",
        "        \n",
        "        print(\"-\" * 60)\n",
        "        print(f\"\\nTotal: {len(available_models)} models available\")\n",
        "        print(\"\\nTo select a model, set MODEL_NAME to one of the labels above.\")\n",
        "        print(\"Example: MODEL_NAME = 'model_1'\")\n",
        "        \n",
        "        # Store the model dictionary globally\n",
        "        MODEL_DICT = model_dict\n",
        "        \n",
        "        # Set default to first model\n",
        "        MODEL_NAME = \"model_1\"\n",
        "        print(f\"\\n‚úì Default model set to: {MODEL_NAME} ({model_dict[MODEL_NAME]})\")\n",
        "        print(\"  Change MODEL_NAME to use a different model.\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ö† No models found. Pull a model first:\")\n",
        "        print(\"  Example: ollama pull llama2\")\n",
        "        print(\"  Example: ollama pull mistral\")\n",
        "        MODEL_NAME = None\n",
        "        MODEL_DICT = {}\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Error listing models: {e}\")\n",
        "    print(\"\\nMake sure Ollama is running:\")\n",
        "    print(\"  Run 'ollama serve' in a terminal, or\")\n",
        "    print(\"  Install Ollama from https://ollama.com\")\n",
        "    MODEL_NAME = None\n",
        "    MODEL_DICT = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Select Your Model\n",
        "\n",
        "Set `MODEL_NAME` to the label of the model you want to use (e.g., `model_1`, `model_2`, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Selected model: model_4 = llama4:latest\n"
          ]
        }
      ],
      "source": [
        "# Select your model by setting MODEL_NAME to one of the labels (model_1, model_2, etc.)\n",
        "# Example: MODEL_NAME = \"model_2\"\n",
        "\n",
        "MODEL_NAME = \"model_4\"\n",
        "\n",
        "# Verify the selected model\n",
        "if 'MODEL_DICT' in globals() and MODEL_DICT and 'MODEL_NAME' in globals() and MODEL_NAME:\n",
        "    if MODEL_NAME in MODEL_DICT:\n",
        "        actual_model_name = MODEL_DICT[MODEL_NAME]\n",
        "        print(f\"‚úì Selected model: {MODEL_NAME} = {actual_model_name}\")\n",
        "    else:\n",
        "        print(f\"‚ö† Warning: '{MODEL_NAME}' not found in available models.\")\n",
        "        print(f\"Available labels: {', '.join(MODEL_DICT.keys())}\")\n",
        "        print(f\"Using default: model_1 = {MODEL_DICT.get('model_1', 'N/A')}\")\n",
        "        MODEL_NAME = \"model_1\"\n",
        "else:\n",
        "    print(\"‚ö† Models not loaded. Run the cell above first!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Setup: Load and Process Website Content\n",
        "\n",
        "We'll create helper functions to read and process HTML files from the mirrored website."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Required packages already installed\n",
            "‚úì Found mirrored website folder: engineering.buffalo.edu\n"
          ]
        }
      ],
      "source": [
        "# Install required packages for HTML processing\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    from bs4 import BeautifulSoup\n",
        "    import html2text\n",
        "    print(\"‚úì Required packages already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing required packages...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"beautifulsoup4\", \"html2text\"])\n",
        "    from bs4 import BeautifulSoup\n",
        "    import html2text\n",
        "    print(\"‚úì Packages installed successfully\")\n",
        "\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "# Set the path to the mirrored website folder\n",
        "MIRROR_FOLDER = \"engineering.buffalo.edu\"\n",
        "\n",
        "# Check if folder exists\n",
        "if not os.path.exists(MIRROR_FOLDER):\n",
        "    print(f\"‚ö† Warning: '{MIRROR_FOLDER}' folder not found in current directory.\")\n",
        "    print(f\"Current directory: {os.getcwd()}\")\n",
        "    print(f\"\\nMake sure you've run the mirroring workflow first.\")\n",
        "else:\n",
        "    print(f\"‚úì Found mirrored website folder: {MIRROR_FOLDER}\")\n",
        "\n",
        "# Initialize HTML to text converter\n",
        "h = html2text.HTML2Text()\n",
        "h.ignore_links = False\n",
        "h.ignore_images = True\n",
        "h.body_width = 0  # Don't wrap text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 10 HTML files (showing first 10)\n",
            "  ‚Ä¢ engineering.buffalo.edu/computer-science-engineering/information-for-faculty-and-staff.html\n",
            "  ‚Ä¢ engineering.buffalo.edu/computer-science-engineering/news-and-events.html\n",
            "  ‚Ä¢ engineering.buffalo.edu/computer-science-engineering/sitemap.html\n",
            "  ‚Ä¢ engineering.buffalo.edu/computer-science-engineering/research.html\n",
            "  ‚Ä¢ engineering.buffalo.edu/computer-science-engineering/alumni-and-friends.html\n"
          ]
        }
      ],
      "source": [
        "def extract_text_from_html(file_path):\n",
        "    \"\"\"Extract readable text from an HTML file.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            html_content = f.read()\n",
        "        \n",
        "        # Use BeautifulSoup to parse and clean HTML\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        \n",
        "        # Remove script and style elements\n",
        "        for script in soup([\"script\", \"style\", \"meta\", \"link\"]):\n",
        "            script.decompose()\n",
        "        \n",
        "        # Convert to text\n",
        "        text = h.handle(str(soup))\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error reading file: {e}\"\n",
        "\n",
        "def find_html_files(root_dir, max_files=None):\n",
        "    \"\"\"Find all HTML files in the directory.\"\"\"\n",
        "    html_files = []\n",
        "    root_path = Path(root_dir)\n",
        "    \n",
        "    for html_file in root_path.rglob(\"*.html\"):\n",
        "        html_files.append(str(html_file))\n",
        "        if max_files and len(html_files) >= max_files:\n",
        "            break\n",
        "    \n",
        "    return html_files\n",
        "\n",
        "# Test: Find some HTML files\n",
        "if os.path.exists(MIRROR_FOLDER):\n",
        "    html_files = find_html_files(MIRROR_FOLDER, max_files=10)\n",
        "    print(f\"Found {len(html_files)} HTML files (showing first 10)\")\n",
        "    for f in html_files[:5]:\n",
        "        print(f\"  ‚Ä¢ {f}\")\n",
        "else:\n",
        "    print(\"Cannot find HTML files - mirror folder not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Query Functions\n",
        "\n",
        "Create functions to query the website content using Ollama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to get current model name\n",
        "def get_current_model():\n",
        "    \"\"\"Get the actual model name from MODEL_NAME label using MODEL_DICT.\"\"\"\n",
        "    try:\n",
        "        if 'MODEL_NAME' in globals() and MODEL_NAME and 'MODEL_DICT' in globals() and MODEL_DICT:\n",
        "            if MODEL_NAME in MODEL_DICT:\n",
        "                return MODEL_DICT[MODEL_NAME]\n",
        "            else:\n",
        "                # Fallback to model_1 if invalid label\n",
        "                if 'model_1' in MODEL_DICT:\n",
        "                    return MODEL_DICT['model_1']\n",
        "    except:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "def query_file_with_ollama(file_path, question, model_name=None, max_chars=8000):\n",
        "    \"\"\"Query a specific HTML file using Ollama.\"\"\"\n",
        "    # Use MODEL_NAME if model_name not provided\n",
        "    if model_name is None:\n",
        "        model_name = get_current_model()\n",
        "        if model_name is None:\n",
        "            return \"Error: No model selected. Please set MODEL_NAME to a model label (e.g., 'model_1').\"\n",
        "    \n",
        "    # Extract text from HTML\n",
        "    file_text = extract_text_from_html(file_path)\n",
        "    \n",
        "    # Truncate if too long (to avoid token limits)\n",
        "    if len(file_text) > max_chars:\n",
        "        file_text = file_text[:max_chars] + \"... [truncated]\"\n",
        "    \n",
        "    # Create prompt\n",
        "    prompt = f\"\"\"You are analyzing content from a university website. Answer the question based on the following content.\n",
        "\n",
        "Content from {file_path}:\n",
        "{file_text}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    \n",
        "    try:\n",
        "        response = ollama.generate(model=model_name, prompt=prompt)\n",
        "        return response['response']\n",
        "    except Exception as e:\n",
        "        return f\"Error querying model: {e}\"\n",
        "\n",
        "def search_and_query(query_text, model_name=None, max_files=5, max_chars_per_file=4000):\n",
        "    \"\"\"Search for relevant files and query them.\"\"\"\n",
        "    # Use MODEL_NAME if model_name not provided\n",
        "    if model_name is None:\n",
        "        model_name = get_current_model()\n",
        "        if model_name is None:\n",
        "            return [{\"file\": \"Error\", \"answer\": \"No model selected. Please set MODEL_NAME to a model label (e.g., 'model_1').\"}]\n",
        "    \n",
        "    if not os.path.exists(MIRROR_FOLDER):\n",
        "        return [{\"file\": \"Error\", \"answer\": \"Mirror folder not found\"}]\n",
        "    \n",
        "    # Find HTML files\n",
        "    html_files = find_html_files(MIRROR_FOLDER)\n",
        "    \n",
        "    # Simple keyword-based search (you could enhance this with better search)\n",
        "    query_lower = query_text.lower()\n",
        "    relevant_files = []\n",
        "    \n",
        "    for file_path in html_files:\n",
        "        file_lower = file_path.lower()\n",
        "        # Check if query keywords appear in filename or path\n",
        "        if any(keyword in file_lower for keyword in query_lower.split()):\n",
        "            relevant_files.append(file_path)\n",
        "            if len(relevant_files) >= max_files:\n",
        "                break\n",
        "    \n",
        "    # If no files found by filename, use first few files\n",
        "    if not relevant_files:\n",
        "        relevant_files = html_files[:max_files]\n",
        "    \n",
        "    print(f\"Found {len(relevant_files)} relevant files to query\")\n",
        "    \n",
        "    results = []\n",
        "    for file_path in relevant_files:\n",
        "        print(f\"  Querying: {file_path}\")\n",
        "        file_text = extract_text_from_html(file_path)\n",
        "        \n",
        "        if len(file_text) > max_chars_per_file:\n",
        "            file_text = file_text[:max_chars_per_file] + \"... [truncated]\"\n",
        "        \n",
        "        prompt = f\"\"\"Based on the following content from a university website, answer: {query_text}\n",
        "\n",
        "Content:\n",
        "{file_text}\n",
        "\n",
        "Answer:\"\"\"\n",
        "        \n",
        "        try:\n",
        "            response = ollama.generate(model=model_name, prompt=prompt)\n",
        "            results.append({\n",
        "                'file': file_path,\n",
        "                'answer': response['response']\n",
        "            })\n",
        "        except Exception as e:\n",
        "            results.append({\n",
        "                'file': file_path,\n",
        "                'answer': f\"Error: {e}\"\n",
        "            })\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Query the Website\n",
        "\n",
        "Now you can query the mirrored website. Try asking questions about:\n",
        "- Faculty members\n",
        "- Research areas\n",
        "- Academic programs\n",
        "- News and events\n",
        "- Department information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: What are the main research areas in the Computer Science department?\n",
            "Model: llama4:latest\n",
            "======================================================================\n",
            "Found 3 relevant files to query\n",
            "  Querying: engineering.buffalo.edu/computer-science-engineering/information-for-faculty-and-staff.html\n",
            "  Querying: engineering.buffalo.edu/computer-science-engineering/news-and-events.html\n",
            "  Querying: engineering.buffalo.edu/computer-science-engineering/sitemap.html\n",
            "\n",
            "--- Result 1 ---\n",
            "File: engineering.buffalo.edu/computer-science-engineering/information-for-faculty-and-staff.html\n",
            "Answer:\n",
            "Error: llama runner process has terminated: signal: killed (status code: 500)\n",
            "\n",
            "\n",
            "--- Result 2 ---\n",
            "File: engineering.buffalo.edu/computer-science-engineering/news-and-events.html\n",
            "Answer:\n",
            "Error: llama runner process has terminated: signal: killed (status code: 500)\n",
            "\n",
            "\n",
            "--- Result 3 ---\n",
            "File: engineering.buffalo.edu/computer-science-engineering/sitemap.html\n",
            "Answer:\n",
            "Error: llama runner process has terminated: signal: killed (status code: 500)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example query - modify this to ask your own questions\n",
        "question = \"What are the main research areas in the Computer Science department?\"\n",
        "\n",
        "# Get current model\n",
        "current_model = get_current_model()\n",
        "print(f\"Query: {question}\")\n",
        "print(f\"Model: {current_model}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Search and query (will use MODEL_NAME if model_name not specified)\n",
        "results = search_and_query(question, max_files=3)\n",
        "\n",
        "# Display results\n",
        "for i, result in enumerate(results, 1):\n",
        "    print(f\"\\n--- Result {i} ---\")\n",
        "    print(f\"File: {result['file']}\")\n",
        "    print(f\"Answer:\\n{result['answer']}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Query a Specific File\n",
        "\n",
        "If you know which file you want to query, you can query it directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Query a specific file\n",
        "# Modify the file path and question as needed\n",
        "\n",
        "specific_file = \"engineering.buffalo.edu/computer-science-engineering/sitemap.html\"\n",
        "question = \"What pages are listed in this sitemap?\"\n",
        "\n",
        "if os.path.exists(specific_file):\n",
        "    current_model = get_current_model()\n",
        "    print(f\"Querying file: {specific_file}\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Model: {current_model}\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Will use dropdown model if model_name not specified\n",
        "    answer = query_file_with_ollama(specific_file, question)\n",
        "    print(answer)\n",
        "else:\n",
        "    print(f\"File not found: {specific_file}\")\n",
        "    print(\"\\nAvailable files in the mirror folder:\")\n",
        "    if os.path.exists(MIRROR_FOLDER):\n",
        "        sample_files = find_html_files(MIRROR_FOLDER, max_files=10)\n",
        "        for f in sample_files:\n",
        "            print(f\"  ‚Ä¢ {f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Interactive Query Function\n",
        "\n",
        "Use this cell to easily query the website with your own questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask_question(question, model_name=None, max_files=5):\n",
        "    \"\"\"Convenience function to ask a question about the website.\"\"\"\n",
        "    # Use dropdown value if model_name not provided\n",
        "    if model_name is None:\n",
        "        model_name = get_current_model()\n",
        "    \n",
        "    print(f\"üîç Question: {question}\")\n",
        "    print(f\"ü§ñ Model: {model_name}\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    results = search_and_query(question, model_name=model_name, max_files=max_files)\n",
        "    \n",
        "    print(f\"\\nüìä Found {len(results)} results\\n\")\n",
        "    \n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Result {i} from: {result['file']}\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(result['answer'])\n",
        "        print()\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Example usage - modify the question below\n",
        "# The function will automatically use the model selected in the dropdown\n",
        "# results = ask_question(\"Who are the faculty members in the Computer Science department?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "- **Model Selection**: Set `MODEL_NAME` to one of the model labels (model_1, model_2, etc.) listed in Step 3. Make sure you've pulled the model you want to use with `ollama pull <model-name>`\n",
        "- **Performance**: Larger models may be slower but provide better answers\n",
        "- **Token Limits**: Very long HTML files may be truncated to stay within model limits\n",
        "- **File Search**: The current search is keyword-based on filenames. For better results, you might want to:\n",
        "  - Use a vector database (like ChromaDB or FAISS) for semantic search\n",
        "  - Implement full-text search across all HTML content\n",
        "  - Use embeddings to find most relevant files before querying\n",
        "\n",
        "### Tips for Better Queries\n",
        "\n",
        "1. Be specific: \"What research does Professor X do?\" is better than \"Tell me about research\"\n",
        "2. Use context: Mention the department or area you're interested in\n",
        "3. Try different models: Change `MODEL_NAME` to test different models (e.g., `MODEL_NAME = \"model_2\"`)\n",
        "4. Query specific files: If you know the file, query it directly for faster results"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "AI",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
